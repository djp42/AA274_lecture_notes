\documentclass[twoside]{article}

\usepackage[math]{kurier}
\usepackage[sc]{mathpazo}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref} % for url
\renewcommand{\sfdefault}{kurier}


\usepackage{graphics}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}


\newcommand{\lecture}[3]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf \sffamily AA 274: Principles of Robotic Autonomy
                        \hfill Winter 2018} }
       \vspace{4mm}
       \hbox to 6.28in { {\sffamily{\Large \hfill Lecture #1: #2  \hfill}} }
       \vspace{2mm}
       % \hbox to 6.28in { {\it \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace*{4mm}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%
%document
\begin{document}
%modify this
\lecture{6}{Stereo Vision and Structure from Motion}{}

\section{Optimal vector m derivation : SVD}
In the previous class, the following equation was derived : 
\begin{equation} \label{eqno:1}
  \tilde{P}m = 0
\end{equation}

There are 11 unknowns which are captured in the vector m. The shape of m is (12,1). If data is available for n correspondences between pixel coordinates and their coordinates in world frame, the shape of $\tilde{P}$ will be (2n,12).  
Since there are 11 unknowns, to solve the system of equations, 6 correspondences are required (since each correspondence gives 2 equations). To avoid noise in data and other inaccuracies, it is desirable to consider more correspondences. Higher the number of correspondences, lower will be the impact of noise on the parameter fitting process. 

However, having more than 6 correspondences will result in 'over constrained' problem. Ideally, if there is no noise in the system, $\tilde{P}m$ will still be 0, because the system of equations will be linearly dependent on the 12 basis equations. 

Because of noise, the objective now is to find m such that the vector $\tilde{P}m$ is closest to 0. Thus, we are effectively trying to minimize the residual (i.e. $\vec{\tilde{P}m}-\vec{0}$) of the norm. Minimization problems corresponding to norm are easier to solve if the norm is differentiable; since the 2 norm is differentiable, the objective is to minimize the 2 norm of $ \tilde{P}m $  vector. Further, we are interested in non trivial solution for m. Since the scaling of m does not matter, we would like our problem be subject to the constraint 2 norm of m being 1. 

Thus, mathematically, speaking our objective is: 

\begin{equation} \label{eqno:2}
  \min_{m\in R^{12}} ||{\tilde{P}m}||^2 
\end{equation}

subject to 

\begin{equation} \label{eqno:3}
||m||^2 = 1
\end{equation}

Following the approach taken in solving constrained optimization problems in the previous sections of the course, the first step is to form a Lagrangian and then try to optimize the Lagrangian. Since two norm of vector x can also be written as $x^T x$, the Lagrangian to the optimization problem we are interested in becomes:

\begin{equation} \label{eqno:4}
 L(m,\lambda) = m^T \tilde{P}^T \tilde{P}m + \lambda (1-(m^T m))
\end{equation}

where lambda is the Lagrange multiplier. By taking gradient with respect to m and and lambda equating them to 0 , the following equations can be written.


\begin{equation} \label{eqno:5}
\nabla_m{L(m^*,\lambda^*)} = 2 \tilde{P}^T \tilde{P} m^* - 2 \lambda m^* = 0
\end{equation}

\begin{equation} \label{eqno:6}
\nabla_\lambda{L(m^*,\lambda)} = 1-  m^*^T m^* = 0
\end{equation}


Rearranging the terms of the equations, we have 

\begin{equation} \label{eqno:7}
\tilde{P}^T \tilde{P} m^* =  \lambda m^*  \end{equation}

Thus, $m^*$ and $\lambda^* $ are eigenvector and eigenvalue respectively of the matrix $ \tilde{P}^T \tilde{P}$ . Further, substituting back the values of  $m^*$ and $\lambda^* $ into the cost function L, we have 

\begin{equation} \label{eqno:8}
L = m^*^T(\tilde{P}^T \tilde{P} m)+ \lambda^* (1-m^*^T m^*) = m^*^T(\lambda^* m^*) + 0 = \lambda^*
\end{equation}

Since we are interested in minimizing L, we would like to pick the smallest eigen-value of matrix $\tilde{P}^T \tilde{P}$. Eigen-values, eigen-vectors come in pairs. Thus, to minimize our cost function L, the eigen-vector corresponding to the smallest vector of the matrix $\tilde{P}^T \tilde{P}$ will give the optimal vector m. 

The construction of matrix $\tilde{P}$ has already been discussed in the previous class. For the matrix $\tilde{P}$, if we can find the smallest eigen-vector corresponding to the matrix $\tilde{P}^T \tilde{P}$, our problem of finding optimal vector m which contains the parameters corresponding to camera is solved. The desired smallest eigen-vector can be found using SVD.

Using Singular Value Decomposition method, any m X n real matrix can be written as 
\begin{equation} \label{eqno:9}
\tilde{P} = U \Sigma V^T \end{equation}

where U and V are orthogonal matrices of shape m X m and n X n respectively and $\Sigma $ is a diagonal matrix. Since U and V are orthogonal, $U^TU = V^TV = I$.  As a result, we have

\begin{equation} \label{eqno:10}
\tilde{P}^T \tilde{P} V = (U \Sigma V^T)^T (U \Sigma V^T) V = V \Sigma^T U^T U \Sigma V = V \Sigma \Sigma  \end{equation}

Further, since,  $\Sigma$ is a diagonal matrix, any column vector v belonging to matrix V follows the following equation,

\begin{equation} \label{eqno:10}
\tilde{P}^T \tilde{P} \vec{v} = \sigma^2 \vec{v}  \end{equation}

where $\sigma$ is the corresponding non zero element in the matrix $\Sigma$. Thus, vector v which belongs to column space of matrix V is an eigen-vector of matrix $\tilde{P}\tilde{P}$. By finding the smallest $\sigma$ in the $\Sigma$ matrix and finding the corresponding column in the matrix V, the desired optimal vector m can be found.

Once the optimal vector m is found, it can be factorized from 12X1 row vector to 3X4 matrix such that M = [KR Kt] where K is an upper triangular matrix and R is a rotation matrix. By doing RQ factorization of first 3 columns of $\tilde{P}$, K and R can be determined. Since K is known, by pre-multiplying the fourth column with inverse of K, t can be determined. Thus, the parameters corresponding to camera are determined

\section{Introduction}

	This lecture \cite{slides} covers geometric concepts needed for 3D reconstruction.  The common methods for reconstructing a 3D scene from 2D images are :
\begin{enumerate}
\item Recognition of landmarks
\item Depth from focus
\item Stereo vision
\item Structure from motion.
\end{enumerate}

	Recognition of landmarks utilizes known characteristics of particular structures such as orthogonal walls. Depth from focus determines distance to a point by taking multiple images with progressively better focus. Stereo vision triangulates the distance of a point in 3D by its coordinates in two distinct images taken at the same time by cameras with known relative pose. Structure from motion processes multiple images taken at different times. A key aspect of stereo vision and structure from motion is finding point correspondences across multiple images. Much of this lecture covers geometric constraints used to narrow the search space for finding point correspondences. Complementary readings \cite{SNS, FP}

\section{Related Works}
Stereo Vision is widely used throughout the robotics community. As such, there are many additional resources to reference if desired.
Below, we include a list of a few of those resources:
\begin{enumerate}
	\item \textit{Computer Vision: A Modern Approach}~\cite{FP}
	\item OpenCV, the open source project for all things computer vision, has some pretty good tutorials on stereo vision. 
	Specifically, there are tutorials for Camera Calibration, Pose Estimation, Epipolar Geometry, and Creating a Depth Map from Stereo Images. 
	These can all be accessed at~\url{https://docs.opencv.org/3.1.0/d9/db7/tutorial_py_table_of_contents_calib3d.html}.
\end{enumerate}
There are also many other techniques that are being investigated in the academic community for other methods of determining distance to objects from images. We learn about a few of them in this lecture, but there are many others:
\begin{enumerate}
	\item Learning based approaches~\cite{Saxena_depth_est}.
	\item Neural Networks for depth estimation~\cite{nips_depth,single_image_depth,Schennings1167554}.
	\item Using assumptions of the world~\cite{effecient_exact,inside_box}.
\end{enumerate}

\section{Method \#1 Depth from Focus}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[scale=0.2]{thinlens}
  \end{center}
  \caption{Projection of a point through a thin lens}
  \label{lens}
\end{figure}

An ideal thin lens (Figure \ref{lens}) focuses rays of light emitting from a point $P$ at a depth $Z$ from the center of the lens to a point $p$ at a distance $z$ from the center of the lens. The distance $z$ is called the focal distance. By properties of similar triangles it can be shown that such points must obey the thin lens constraint (Equation \ref{tlc}).
\begin{align}
  \label{tlc}
  \frac{1}{z} + \frac{1}{Z} = \frac{1}{f}
\end{align}

Thus if the focus distance $z$ and focal length $f$ are known, the depth, $Z$ of the point $P$ can be obtained. We assume the focal length is known as a property of the lens. The focus distance $z$ is found by trying different values until the projection of the point in question becomes sharp in the image.

In practice this technique is not often used because different focus distances must be found for every point in the image which is not at the same depth.

\section{Method \#2 Stereopsis}

In practice, a more powerful technique used to reconstruct 3D scenes from 2D images is the process of fusing two images into one, known as stereopsis. This is how humans see. And indeed it has been demonstrated to be effective in robotics, this is on what the vision of Mars rovers is based.

In principle, the depth of $P$ can be obtained from the intersection of two lines of sight (Figure \ref{triangle}). In other words, given $\overline{Op}$ and $\overline{O'p'}$ the point $P$ is determined by the intersection of these lines and this process is called triangulation. To know $\overline{Op}$ and $\overline{O'p'}$ in a  world coordinate frame fixed for one of the cameras, the relative pose of the cameras and their intrinsic matrices must be known.

\begin{figure}[h!]
  \begin{center}
	\begin{tabular}{cc}
	  \includegraphics[scale=0.3]{triangulation.png}
	  \includegraphics[scale=0.3]{approximation.png}
	\end{tabular}
  \end{center}
  \caption{The triangulation principle in 2D (left), the approximation in 3D (right)}
  \label{triangle}
\end{figure}

Due to noise in the measurement and calibration it is essentially impossible for the two lines of sight to precisely intersect in 3D (Figure \ref{triangle}). Since there is no single point that can exactly correspond to the observations, in practice, $P$ is approximated by finding the 3D-point $Q$ that best explains the observations. I.e. the point whose projection in the two image planes is closest to the observed points. This is computed in term of the following metric known as the re-projection error :
\begin{align}
Q = \arg\!\min_Q (d^2(p, q) + d^2(p', q'))
\end{align}

where $q$ and $q'$ are the projection of $Q$ onto the two camera images planes and $p$ and $p'$ are observed point correspondences in the image planes.
\section{Method \#2 Stereopsis}

In practice, a more powerful technique used to reconstruct 3D scenes from 2D images is the process of fusing two images into one, known as stereopsis. This is a process used by humans to see, and it has also been demonstrated to be effective in robotics, as this is what the Mars rovers vision systems are based on.

In principle, the depth of $P$ can be obtained from the intersection of two lines of sight (Figure \ref{triangle}). In other words, P is the intersection point of $\overline{Op}$ and $\overline{O'p'}$. This process is called triangulation. In order to know $\overline{Op}$ and $\overline{O'p'}$ in a  world coordinate frame fixed for one of the cameras, the relative pose of the cameras and their intrinsic matrices must be known.

\begin{figure}[h!]
  \begin{center}
	\begin{tabular}{cc}
	  \includegraphics[scale=0.3]{triangulation.png}
	  \includegraphics[scale=0.3]{approximation.png}
	\end{tabular}
  \end{center}
  \caption{The triangulation principle in 2D (left), the approximation in 3D (right)}
  \label{triangle}
\end{figure}

Due to noise in the measurement and calibration, it is nearly impossible for the two lines of sight to precisely intersect in 3D (Figure \ref{triangle}). Since there is no single point that can exactly correspond to the observations, in practice $P$ is approximated by finding the 3D-point $Q$ that best explains the observations (i.e. the point whose projection in the two image planes is closest to the observed points). This is computed in term of the following metric known as the re-projection error :
\begin{align}
Q = \arg\!\min_Q (d^2(p, q) + d^2(p', q'))
\end{align}

where $q$ and $q'$ are the projection of $Q$ onto the two camera image planes and $p$ and $p'$ are observed point correspondences in the image planes.

\subsection{Stereo vision process}

So far we have seen that given corresponding projected points in two images from a calibrated camera, we can reconstruct the coordinates of the original point in 3D. This is actually the second step of the two-step stereo vision process :

\begin{enumerate}
  \item Correspondence: fusion of features observed by two (or more) cameras
  \item Triangulation: reconstruction of three-dimensional pre-images
\end{enumerate}

The first step is finding corresponding points in the two images. In practice this is the more challenging step of the two. In the general case, finding correspondences requires a search in two dimensions between all points in each image, but several constraints can be leveraged to restrict this search, such as similarity constraints, continuity constraints, and most importantly the \emph{epipolar} constraint.

The epipolar constraint intrinsically specifies the geometry of stereo vision. Specifically, given a point in one image, the epipolar constraint limits the location of the corresponding point in the other image to a line, thus reducing the search space for correspondence from two dimensions to one.

\subsection{Epipolar geometry}

Consider the following problem: we know the relative positions and orientations of two calibrated cameras. We have an observation of a point $p$ in our first image plane, and we want to find a constraint that helps find the corresponding point $p'$ in the second image plane, where $p$ and $p'$ are projections of the same point $P$ in the 3D.

We start by defining the \emph{epipolar plane} as the plane that passes through $O$, $O'$, and $p$. Since $p$ and $p'$ are on the lines $\overline{OP}$ and $\overline{O'P}$, we know that $P$ and $p'$ also lie on the epipolar plane. Now consider virtual image planes $\Pi$ and $\Pi'$ in front of the camera centers. These planes are known since we assumed our cameras are calibrated. Since $p'$ must lie both on the epipolar plane and the virtual plane $\Pi'$, then $p'$ must lie on the intersection of the epipolar plane and the virtual image plane. This intersection line is called the \emph{epipolar line}. Thus, given $O$, $O'$, $p$, and the image plane $\Pi'$, we have constrained the location of the corresponding point $p'$ to a line in $\Pi'$.

In the figure \ref{epi}, note that $O$, $O'$, and $p$ are known. $P$ is not known precisely, but we must be somewhere along $\overline{Op}$. Then, the line of sight from the second camera to $P$ must pass through $O'$ and the line $\overline{Op}$. These candidate lines are shown in red in figure \ref{epi}. These candidate lines cross the image plane $\Pi'$ at points that form the epipolar line.

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.3]{epipolar.png}
  \end{center}
  \caption{The epipolar plane}
  \label{epi}
\end{figure}

Summary of terminology:
\begin{itemize}
\item \textbf{Epipolar plane}: the plane containing the points O, O', p, p', and P
\item \textbf{Baseline}: the line connecting the centers, $\overline{OO'}$
\item \textbf{Epipolar lines}: the intersection of the epipolar plane and the image planes, respectively $l$ and $l'$
\item \textbf{Epipoles}: the intersection points of the baseline and the virtual image planes, respectively $e$ and $e'$
\item \textbf{Epipolar constraint}: potential correspondences for $p$ must lie on the epipolar line $l'$ (and vice versa)
\end{itemize}

Figure \ref{correspondence} shows an example of corresponding points from two images and the epipolar line.

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.3]{correspondence2.jpg}  \end{center}
  \caption{An example of computed epipolar plane (left) from an initial point (right)}
  \label{correspondence}
\end{figure}

\subsubsection{Epipolar geometry and the essential matrix}

To define the epipolar constraint mathematically, we use the fact that $\overline{Op}$, $\overline{O'p'}$, and $\overline{OO'}$ are coplanar to write:

\begin{align}
\overline{Op} \cdot [\overline{OO'} \times \overline{O'p'}] = 0.
\label{perpendicular}
\end{align}

To understand why this is true, note that $\overline{OO'} \times \overline{O'p'}$ is a vector perpendicular to the epipolar plane, so this vector must be perpendicular to $\overline{Op}$. Since the dot product of perpendicular vectors is 0, equation \ref{perpendicular} is true. 

Now assume that the world reference system is co-located with camera 1. Define normalized coordinates $\hat{p} = K^{-1}p$ and $\hat{p'} = K'^{-1}p'$. Define the rotation matrix between camera 1 and camera 2 as $R$ such that in the camera 1 reference frame, $\overline{O\hat{p'}} = R\hat{p'}$. Define the translation from camera 1 to camera 2 as $t$ so $\overline{OO'} = t$. Then our coplanar constrain becomes

\begin{align}
\hat{p} \cdot [t \times (R\hat{p'})] = 0.
\end{align}

Note that a cross product can be expressed as the product of a skew-symmetric matrix and a vector.

\begin{align}
a \times b &= [a]_x b \\
\text{where } [a]_x &= \begin{bmatrix}
0 & -a_3 & a_2 \\
a_3 & 0 & -a_1 \\
-a_2 & a_1 & 0
\end{bmatrix} .
\end{align}

Then the epipolar constraint can be written as a product of matrices as :
\begin{align}
\hat{p} \cdot [t \times (R\hat{p'})] &= \hat{p} [t]_x R \hat{p'} = \hat{p}E\hat{p'} = 0. \\
\text{where } E &= [t]_xR.
\end{align}

We now have a single matrix that defines the epipolar constraint for any normalized point correspondences between two calibrated cameras. Note that a line in homogeneous coordinates is defined by the points that satisfy $p \cdot l = 0$, so the epipolar lines can be calculated as:
\begin{align}
l &= E\hat{p'} \\
l' &= E^T\hat{p}.
\end{align}

For the special case of the epipoles the epipolar line is undefined because $t$ and the epipoles are parallel so $E^Te = Ee' = 0$. This leads to the result that $E$ is singular (non-invertible). Finally, we make note that $E$ has five degrees of freedom, corresponding to 3 for $R$ and 2 for $t$ due to scaling ambiguity.

\subsubsection{Epipolar geometry and the Fundamental Matrix \cite{FP}}

To generalize the essential coordinates to native coordinates (as opposed to normalized coordinates), we note that $p = K\hat{p}$ and $p' = K'\hat{p'}$. The epipolar constraint then becomes

\begin{align}
p^TK^{-T}EK'^{-1}p' &= p^T F p' = 0 \\
\text{where } F &= K^{-T}E K'^{-1} = K^{-T}[t]_x R K'^{-1}.
\end{align}

Once again we have epipolar lines as
\begin{align}
l &= Fp' \\
l' &= F^Tp.
\end{align}

$F$ is also singular because $F^Te = Fe' = 0$. Finally $F$ is defined by 9 elements but is constrained by det(F) = 0 and a common scaling so in the end $F$ has only 7 degrees of freedom.

\subsubsection{Computing the Fundamental Matrix}

Given the fundamental matrix $F$, we can compute a specific epipolar line in image 2 that corresponds to a point $P$ in image 1 without any additional information, as shown in Figure \ref{ex2}

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.45]{correspondence3.PNG}  \end{center}
  \caption{An example of computed epipolar line and point $p'$ that correspond to a point $p$ (left)}
  \label{ex2}
\end{figure}

To compute a fundamental matrix, we use the same approach as we did for projection matrix. We find a number of correspondences, for example by Brute Force search, using clearly distinguishable points such as corners.

Assume we are given a number of corresponding points $p = [u, v, 1]^T$ and $p'= [u',v',1]^T$ expressed in a homogeneous coordinates. Each pair of points has to satisfy the epipolar constraint $F$.

\begin{align}
[u, v, 1] \begin{bmatrix}
F_{11} & F_{12} & F_{13} \\
F_{21} & F_{22} & F_{23} \\
F_{31} & F_{32} & F_{33}
\end{bmatrix} \begin{bmatrix}
u' \\ v' \\ 1
\end{bmatrix} &= 0
\end{align}

We can rewrite this into a scalar product of two one-dimensional vectors. The first vector contains the known coefficients from the given points, and the second vector contains each element of the fundamental matrix. Now we have a scalar equation, therefore one constraint, for each pair of given points.

\begin{align}
[uu', uv', u, vu', vv', v, u', v', 1]\begin{bmatrix}
F_{11} \\F_{12} \\F_{13} \\F_{21} \\F_{22} \\F_{23} \\F_{31} \\F_{32} \\F_{33}
\end{bmatrix} = Wf = 0
\end{align}

As in the projection matrix, the fundamental matrix can be defined up to scale, e.g. normalized by the last component. Therefore, we need 8 parameters to estimate 9 entries of the fundamental matrix. Given $n>=8$ correspondences, we can solve $\tilde{F}$

\begin{align}
\min_{f\in{R^9}}\|Wf\|^2 \\
\text{subject to } \|f\|^2 = 1
\end{align}

Here, while $\tilde{F}$ satisfies the epipolar constraints, it is not necessarily singular. we can enforce rank-2 constraint via SVD decomposition to get the proper fundamental matrix from the candidates.

\begin{align}
\arg\!\min_F \|F-\tilde{F}\|^2 \\
\text{subject to } det(F) = 0
\end{align}

\subsubsection{Image Rectification}

If we assume we have two parallel image planes, the triangulation and correspondence calculations are much simpler because the epipolar lines are now horizontal and the $v$ coordinates are equal. We can warp images to simulate a parallel image plane through a process called "Image Rectification" (Figure \ref{rect}). This process exploits some algorithms applying projective transformation, which will not be discussed in this lecture. So let's assume we have rectified image pairs.

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.5]{rectified_image.PNG}  \end{center}
  \caption{Image Rectification}
  \label{rect}
\end{figure}

Recall that stereo vision consists of two steps: correspondence and triangulation. This lecture will focus on the triangulation problem. A preimage point $P$ is projected on two image planes to be $p_u$ and $p'_u$. $u$ is the horizontal coordinate and $v$ is the vertical coordinate. Because the epipolar line is horizontal, $p_u$ and $p'_u$ have the same vertical coordinate, and we do not consider $v$ coordinates.

Knowing the horizontal projection of $p$ into two cameras, we can estimate the distance $z$ of the point from the similar triangles (Figure \ref{recttri}).

\begin{align}
%   \label{tlc}
  \frac{z}{b} &= \frac{z-f}{b-p_u+p'_u} \\
  \ z &= \frac{bf}{p_u-p'_u}
\end{align}

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.45]{rectified_triangulation_1_.PNG}
	\includegraphics[scale=0.4]{rectified_triangulation_2_.PNG}  \end{center}
  \caption{Triangulation under Rectified Images (horizontal view on the left, top-down view on the right)}
  \label{recttri}
\end{figure}

Here $b$ is the baseline and $f$ is the focal length. If the baseline is very large, we have more resolution to estimate the distance $z$, but some points may only be visible from one camera. If the baseline is small, we do not have problems in correspondences, but we will have a larger relative error in the estimation of $z$.

We can measure the distance by knowing the disparity $p_u-p'_u$ (ie the pixel displacement between corresponding points). From the two images, we can build a disparity map, an image that plots the disparity values for every pixel from two stereo images (See Fig. 6.8.) Brighter color indicates larger disparities, which means the point is closer to the camera.

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.5]{disparity_map.PNG}  \end{center}
  \caption{Disparity Map from a pair of Stereo Images}
  \label{ex}
\end{figure}


\subsection{Stereo Fusion Techniques}
Given epipolar lines, several methods exist to solve for the corresponding coordinates along these lines.  

\subsubsection{Local Methods for Stereo Fusion}
One subset of stereo fusion methods search for corresponding points using only local information around the candidate points. One such method is the Correlation Method which is applied to rectified images and compares the intensities of pixels in the neighborhood of potential coordinates. 

Consider a window of size $p = (2m+1)\cdot(2n+1)$ centered around (u,v) in image 1, as shown in figure \ref{CorrelationWindow}. We can associate with this window the vector $w(u,v) \in \mathbb{R}^{p}$ which is calculated based on the intensity of the pixels in each row of the window. Let the second window be centered around (u',v'). Because image 1 and image 2 are parallel, the second window is translated from the first window by an offset $d$. We can calculate the vector $w'(u+d,v)$ associated with this second window and define the normalized correlation function between the two vectors as

\begin{align}
C(d) = \frac{1}{||w-\bar{w}||}\frac{1}{||w'-\bar{w'}||}[(w-\bar{w})\cdot(w'-\bar{w'})]
\end{align}

where the coordinates of $\bar{w}$ are all equal to the mean of the coordinates of $w$. The output of this normalized function $C$ ranges from -1 to +1. Matches between corresponding coordinates can be found by finding the maximum of the $C$ function over a range of $d$. 

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.5]{CorrelationWindow.png}  \end{center}
  \caption{Correlation of two windows along an epipolar line (FP)}
  \label{CorrelationWindow}
\end{figure}

Note that finding the maximum of the $C$ function corresponds to calculating the minimum of 

\begin{align}
|\frac{1}{||w-\bar{w}||}(w-\bar{w})-\frac{1}{||w'-\bar{w'}||}(w'-\bar{w'})|^{2}
\end{align}

A significant problem with this correlation method is that it assumes that the observed surface is parallel to the image planes. Figure \ref{ForeshorteningEffect} demonstrates how angled surfaces make the neighborhoods around corresponding points different between the two images. 

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.5]{ForeshorteningEffect.png}  \end{center}
  \caption{The foreshortening of an angled plane demonstrates that a distortion exists between the points on image 1 vs image 2.}
  \label{ForeshorteningEffect}
\end{figure}


Another method called the Multi-Scale Edge Matching method addresses the shortcomings of the correlation method by matching features, such as edges, instead of pixel intensities (see Algorithm 7.1 in FP). 

\subsubsection{Global Methods for Stereo Vision}
A second set of methods for stereo fusion involve using non-local constraints to find correspondences. 

One such constraint is the ordering constraint which assumes that the order of image points mapped from an object onto an epipolar line will always be the same, as shown in the left image of Figure \ref{OrderingConstraint}. However, note that this constraint does not hold if a small object is in front of a larger object. In this case, the order of image points is not the same between the first and second images, as shown in the right image of Figure \ref{OrderingConstraint}. 

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.5]{OrderingConstraint.png}  \end{center}
  \caption{(left) Ordering Constraints (right) The effect of a small object in front of a larger one on the order of image points on epipolar lines.}
  \label{OrderingConstraint}
\end{figure}

The ordering constraints can be used with dynamic programming to create efficient algorithms to find correspondences (see Algorithm 7.2 in FP). 

Another non-local constraint is the smoothness constraint for the energy function $E:\mathbb{D}^n \rightarrow \mathbb{R}$ given by

\begin{align}
E(d) = \sum_{p \in V} U_p(d_p) + \sum_{(p,q) \in E)} B_{pq}(d_p,d_q)
\end{align}

where $d$ is a vector containing $n$ integer disparities $d_p$ corresponding to pixels $p$ and $U_p(d_p)$ calculates the discrepancy between pixel p in the first image and pixel $p+d_p$ in the second one. A pair of corresponding points can be found by minimizing this error function. 


\newpage
\section{Method \#3 Structure From Motion (SFM)}

The Structure From Motion (SFM) method uses a similar principle than Stereopsis but in a different fashion : we do not take images from two cameras at the same time, but instead we take images from one camera at different points in time as we move the camera around the object. Here, the intrinsics parameters are the same because we are using one camera, but the extrinsic changes as we move the camera.

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.4]{SFM.PNG}  \end{center}
  \caption{Structure From Motion (SFM)}
  \label{ex}
\end{figure}

Let's consider we have different points $P_j$ on an object and we take the image at time $k$. Given $m$ images of $n$ fixed 3D points, we get $m$ projection matrices $M_k$ (for the $k$th image) : $p_{j,k}^h = M_k P_j^h$.
Where $p_{j,k}$ is the projection of the point $P_j$ on the camera image $k$.

However, SFM is difficult to use because it has ambiguities. That is, we cannot recover the absolute scale of the observed scene. If we have a bigger object in a longer distance in one situation and a smaller object in a closer distance, the projections will be the same (see Figure \ref{amb}). To address this ambiguity, we can take several approaches such as algebraic approach (by fundamental matrix) and bundle adjustment.

\begin{figure}[h!]
  \begin{center}
	\includegraphics[scale=0.8]{SFM_ambiguities.png}
  \end{center}
  \caption{Ambiguity in the projection of an object using SFM}
  \label{amb}
\end{figure}

The concept of SFM is similar to visual odometry, which estimate the motion of a robot by using visual inputs in series. This is widely used in reality, including on Mars by rovers. SFM can be a very powerful method because it not only allows us to reconstruct the environment but also to recover the motion of the camera.

\newpage

\bibliographystyle{unsrt}
\bibliography{references}

\subsubsection*{Contributors}
Winter 2019: Derek Phillips, Oscar Matus, Sarah Brennan, Kyle Julian, Marion Lepert, Sai Modalavalasa
\\
Winter 2018: Gael Gurvan Colas, Soyeon Jung, Greg Katz

\end{document}
